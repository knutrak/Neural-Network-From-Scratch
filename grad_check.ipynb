{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6f6b3a",
   "metadata": {},
   "source": [
    "***Gradient check***\n",
    "\n",
    "Approximating gradient and comparing with the analytical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa1e1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_loss_BCE(NN, X, y):\n",
    "    m = X.shape[0]\n",
    "    loss = 0.0\n",
    "    for i in range(m):\n",
    "        p = NN.forward_prop(X[i])       # (1,1)\n",
    "        p = float(p[0,0])\n",
    "        yi = float(y[i]) if np.asarray(y[i]).ndim==0 else float(y[i][0])\n",
    "        loss += -(yi*np.log(p) + (1.0-yi)*np.log(1.0-p))\n",
    "    return loss / m\n",
    "\n",
    "def analytical_grad(NN, X, y):\n",
    "    m = X.shape[0]\n",
    "    NN.Delta = NN.init_D()\n",
    "    for i in range(m):\n",
    "        NN.forward_prop(X[i])\n",
    "        d = NN.calculate_small_d(y[i])      \n",
    "        NN.calculate_big_Delta(d)\n",
    "    return [D / float(m) for D in NN.Delta]\n",
    "\n",
    "\n",
    "\n",
    "def grad_check(NN, X, y, epsilon=1e-5):\n",
    "\n",
    "    loss_fn = batch_loss_BCE\n",
    "    analytical = analytical_grad(NN, X, y)\n",
    "    numerical  = [np.zeros(Wl.shape, dtype=float) for Wl in NN.W]\n",
    "    original   = [Wl.copy() for Wl in NN.W]\n",
    "\n",
    "    for l in range(len(NN.W)):\n",
    "        rows, cols = NN.W[l].shape\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                w0 = NN.W[l][r, c]\n",
    "\n",
    "                NN.W[l][r, c] = w0 + epsilon\n",
    "                Jp = loss_fn(NN, X, y)\n",
    "\n",
    "                NN.W[l][r, c] = w0 - epsilon\n",
    "                Jm = loss_fn(NN, X, y)\n",
    "\n",
    "                numerical[l][r, c] = (Jp - Jm) / (2.0 * epsilon)\n",
    "                NN.W[l][r, c] = w0\n",
    "        NN.W[l][:] = original[l]\n",
    "\n",
    "    # simple per-layer report\n",
    "    diffs = []\n",
    "    for l in range(len(NN.W)):\n",
    "        A, N = analytical[l], numerical[l]\n",
    "        max_abs = 0.0\n",
    "        max_rel = 0.0\n",
    "        for r in range(A.shape[0]):\n",
    "            for c in range(A.shape[1]):\n",
    "                d = abs(float(A[r,c]) - float(N[r,c]))\n",
    "                if d > max_abs: \n",
    "                    max_abs = d\n",
    "        \n",
    "        print(f\"Layer {l}: max |E|={max_abs:.3e}\")\n",
    "        diffs.append((max_abs, max_rel))\n",
    "    return diffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ea6836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: max |E|=1.649e-11\n",
      "Layer 1: max |E|=1.347e-11\n",
      "Layer 2: max |E|=1.314e-11\n",
      "Layer 3: max |E|=1.514e-11\n"
     ]
    }
   ],
   "source": [
    "from Model import NeuralNet\n",
    "\n",
    "X_batch = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "y_batch = np.array([[1], [0], [1]])\n",
    "\n",
    "\n",
    "NN_GradCheck = NeuralNet(X_batch.shape[1], [2,2,3],y_batch.shape[1])\n",
    "diffs= grad_check(NN_GradCheck, X_batch, y_batch, epsilon=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09964add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
